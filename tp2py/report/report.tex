\documentclass{article}
\usepackage{multicol}
\usepackage{subfig}

\usepackage{amsmath}
\usepackage{float}
\usepackage{algorithm}
\usepackage{multicol}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage[table]{xcolor}

\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Trabalho Prático II - Introdução à Inteligência Artificial}
\author{Luís Felipe Ramos Ferreira}
\date{\href{mailto:lframos.lf@gmail.com}{\texttt{lframos.lf@gmail.com}}
}

\begin{document}

\maketitle

\section{Introdução}

O Trabalho Prático II da disciplina de Introdução a Inteligência Artificial teve como objetivo a implementação do algoritmo de \texttt{Q-Learning}
para encontrar o melhor caminho entro um ponto inicial e um objetiov em uma mapa bidimensional.

\section{Implementação}

O projeto foi implementado na linguagem Python, versão 3.12.3. Um arquivo \texttt{requirements.txt} com os pacotes utilizados no ambiente virtual criado para desenvolvimento
está disponibilizado. O único pacote fora dos já disponibilizados por padrão na Linguagem foram Numpy, Pandas e Matplotlib. Instruções para rodar o programa estão disponibilizadas no
arquivo \texttt{README.md} disponibilizado.

\section{Q-learning}

O algoritmo de \texttt{Q-Learning} é um algoritmo de aprendizado por reforço, em que o agente, ao explorar o ambiente e interagir com o que ele possui,
passa a aprender mais sobre como alcançar seus objetivos. Para cada possível estado \(s\), ele seleciona a melhor ação \(a\) para se tomar, obtendo assim
o estado \(s'\) que será alcançado ao aplicar \(a\) em \(s\), assim como uma recompensa \(r\) por aplicar essa ação. Por fim, ele atualiza o valor de \(Q(s, a)\),
que é mantido para todo par de estado e ação e indica quão bom é executar aquela ação partindo daquele estado.

O pseudocódigo abaixo, inspirado nos slides da disciplina, indica, em alto nível, como funciona o algoritmo.

\begin{itemize}
	\item Inicializa \(Q(s, a)\) para todos os estados e ações
	\item \(s \gets\) estado inicial
	\item \(n \gets\) número de episódios
	\item \(\epsilon \gets\) limiar \(\epsilon-greedy\)
	\item \(\gamma \gets\) taxa de desconte
	\item \(\alpha \gets\) taxa de aprendizado
	\item Para todo \(i = 0\) até \(n\)
	      \begin{itemize}
		      \item se \(random() < \epsilon: a \gets random\_action()\)
		      \item senão \(a \gets argmax_a(Q(s, a))\)
		      \item Executa ação \(a\), observa recompensa \(r\) e próximo estado \(s'\)
		      \item \(Q(s, a) \gets Q(s, a) + \alpha * [r + \gamma* max_{a' \in A}Q(s', a') - Q(s, a)]\)
		      \item \(s \gets s'\)
	      \end{itemize}
\end{itemize}

Uma característica do \texttt{Q-Learning} é que ele é um algoritmo chamado de \textit{model free}, que em alto nível significa dizer que em nenhum momento o agente
precisa aprender ou estimar a função de transição que ele utiliza. Ele também é um algoritmo \textit{off-policy}, ou seja, a política utilizada pelo agente
na exploração pode ser diferente da política que ele está aprendendo (\textit{target policy}).

O algoritmo possui algumas pequenas variações, as quais exploramos no trabalho, e estão descritas a seguir.

\begin{itemize}
	\item \textbf{STANDARD}: nesta versão, utilizamos o algortimo normalmente, com a tabela de recompensas descrita na especificação.
	\item \textbf{POSITIVE}: também utilizamos a tabela descrita na especificação do trabalho. Nesa versão, toda recompensa é positiva.
	\item \textbf{STOCHASTIC}: nesta versão, após escolher uma ação, o algortimo tem 20\% de chance de na verdade executar outra ação perpendicular à ação escolhida,
	      conforme descrito na especificação.
\end{itemize}

\section{Estruturas e modelagem}

A linguagem \textit{Python} facilitou muito o trabalho. Para modelar as 4 ações possíveis, utilizamos números inteiros. EM particular, para ficar mais organizado,
utilizamos a estrutura \textit{IntEnum}. O mapa é um \textit{array} da biblioteca \textit{numpy}, que armazena caracteres. A matriz de pesos é uma matriz tridimensional,
também \textit{numpy}, que armazena floats. Ela possui as dimensões do mapa de entrada, mas cada posição é composta por um vetor de 4 posições, uma para cada possível ação.

As variações \textbf{STANDARD} e \textbf{POSITIVE} foram implementadas na mesma função, chamada de \textit{qlearning}, pois elas são idênticas a menos do valor das recompensas, que é mudado dinamicamente conforme
os parâmetros de linha de comando. A variação \textbf{STOCHASTIC} é implementada numa função diferente, chamada \textit{stochastic}, para facilitar a organização.

As implementações seguem exatamente as especificações do algoritmo apresentadas na disciplina.

\section{Análise das diferentes políticas}

Para analisar as diferentes políticas, testamos elas com os diferentes mapas disponibilizados no \textit{moodle} da disciplina. Os resultados obtidos foram os seguintes.
Nas tabelas, a célula verde indica o ponto inicial e a célula vermelha indica o ponto onde se encontra o objetivo.

\subsection{Mapa teste}

Para o mapa teste inicial, que pode ser visto abaixo, as políticas encontradas para cada uma das variações foram as seguintes.

\begin{center}
	\[
		\begin{array}{|c|c|c|c|c|}
			\hline
			+ & + & + & . & O \\ \hline
			. & @ & @ & . & x \\ \hline
			. & @ & @ & . & . \\ \hline
			. & ; & ; & ; & . \\ \hline
		\end{array}
	\]
\end{center}

\vspace{10pt}

\begin{multicols}{3}

	\centering \small \textbf{STANDARD}
	\[
		\begin{array}{|c|c|c|c|c|}
			\hline
			>                     & > & > & >      & \cellcolor{red!20}O \\ \hline
			v                     & @ & @ & \hat{} & x                   \\ \hline
			v                     & @ & @ & \hat{} & <                   \\ \hline
			\cellcolor{green!20}> & > & > & \hat{} & \hat{}              \\ \hline
		\end{array}
	\]

	\columnbreak

	\centering \small \textbf{POSITIVE}
	\[
		\begin{array}{|c|c|c|c|c|}
			\hline
			>                     & > & > & <      & \cellcolor{red!20}O \\ \hline
			v                     & @ & @ & \hat{} & x                   \\ \hline
			v                     & @ & @ & >      & v                   \\ \hline
			\cellcolor{green!20}> & > & > & \hat{} & \hat{}              \\ \hline
		\end{array}
	\]

	\columnbreak

	\centering \small \textbf{STOCHASTIC}
	\[
		\begin{array}{|c|c|c|c|c|}
			\hline
			>                          & > & > & >      & \cellcolor{red!20}O \\ \hline
			\hat{}                     & @ & @ & >      & x                   \\ \hline
			\hat{}                     & @ & @ & >      & <                   \\ \hline
			\cellcolor{green!20}\hat{} & > & > & \hat{} & \hat{}              \\ \hline
		\end{array}
	\]

\end{multicols}

A política foi encontrada para o ponto inciial \((0, 3)\), com 10000 episódios. EM particular, para a variação \textbf{POSITIVE}, que demora bastante, utilizamos 500 iterações. Podems ver que todas encontram uma política satisfatória
para encontrar o melhor caminho para o objetivo. Em particular, a variação \textbf{STANDARD} foi a mais rápida e a que, a princípio, encontrou o melhor resultado.

\subsection{Mapa labirinto (\textit{Maze})}

O mapa \textit{Maze} está exposto abaixo.

\[
	\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		x & @ & x & @ & x & @ & x & @ & x & @ & . & @ \\ \hline
		. & . & . & . & . & . & . & . & . & . & . & @ \\ \hline
		. & x & @ & @ & @ & @ & . & @ & . & @ & @ & @ \\ \hline
		. & x & @ & . & . & . & . & . & . & @ & . & @ \\ \hline
		. & x & @ & . & @ & @ & @ & @ & @ & @ & @ & @ \\ \hline
		. & x & @ & . & . & . & . & . & . & @ & . & . \\ \hline
		. & x & @ & @ & @ & @ & @ & @ & @ & @ & @ & . \\ \hline
		O & . & . & . & . & . & . & . & . & . & . & . \\ \hline
	\end{array}
\]

\vspace{10pt}


\begin{center}
	\small \textbf{STANDARD}
	\[
		\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			x                   & @ & x      & @      & x      & @      & x      & @ & x & @ & \cellcolor{green!20}v & @ \\ \hline
			v                   & < & <      & <      & <      & <      & <      & < & < & < & <                     & @ \\ \hline
			v                   & x & @      & @      & @      & @      & @      & @ & @ & @ & \hat{}                & @ \\ \hline
			v                   & x & @      & <      & <      & >      & <      & > & < & > & \hat{}                & @ \\ \hline
			v                   & x & @      & >      & @      & @      & @      & @ & @ & @ & @                     & @ \\ \hline
			v                   & x & @      & \hat{} & <      & <      & \hat{} & v & < & > & >                     & x \\ \hline
			v                   & x & @      & @      & @      & @      & @      & @ & @ & @ & >                     & @ \\ \hline
			\cellcolor{red!20}O & v & \hat{} & >      & \hat{} & \hat{} & v      & < & v & > & >                     & @ \\ \hline
		\end{array}
	\]
\end{center}


\small \textbf{POSITIVE}
\[
	\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		x                   & @      & x & @      & x      & @ & x      & @      & x      & @ & \cellcolor{green!20} v & @ \\ \hline
		v                   & <      & < & <      & <      & < & <      & <      & >      & < & <                      & @ \\ \hline
		v                   & x      & @ & @      & @      & @ & @      & @      & @      & @ & \hat{}                 & @ \\ \hline
		v                   & x      & @ & v      & \hat{} & v & v      & \hat{} & \hat{} & < & \hat{}                 & @ \\ \hline
		v                   & x      & @ & \hat{} & @      & @ & @      & @      & @      & @ & @                      & @ \\ \hline
		v                   & x      & @ & v      & >      & < & \hat{} & >      & \hat{} & v & \hat{}                 & x \\ \hline
		\hat{}              & x      & @ & @      & @      & @ & @      & @      & @      & @ & v                      & @ \\ \hline
		\cellcolor{red!20}O & \hat{} & < & v      & v      & > & v      & \hat{} & v      & < & v                      & @ \\ \hline
	\end{array}
\]

\small \textbf{STOCHASTIC}
\[
	\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		x                   & @ & x & @ & x      & @ & x      & @ & x      & @ & \cellcolor{green!20}v & @ \\ \hline
		v                   & < & < & > & \hat{} & < & \hat{} & > & \hat{} & < & <                     & @ \\ \hline
		v                   & x & @ & @ & @      & @ & @      & @ & @      & @ & \hat{}                & @ \\ \hline
		>                   & x & @ & v & <      & < & <      & v & >      & > & \hat{}                & @ \\ \hline
		v                   & x & @ & v & @      & @ & @      & @ & @      & @ & @                     & @ \\ \hline
		v                   & x & @ & > & >      & > & >      & > & >      & > & >                     & x \\ \hline
		>                   & x & @ & @ & @      & @ & @      & @ & @      & @ & \hat{}                & @ \\ \hline
		\cellcolor{red!20}O & < & < & < & <      & < & <      & < & <      & > & \hat{}                & @ \\ \hline
	\end{array}
\]

Podemos notar nesse mapa maior que, mais uma vez, a variação \textbf{STANDARD} apresentou uma política melhor que a dos demais. No entanto, podemos ver que a performance das outras duas deixou a desejar. Para um número de episódios igual a 30000, a variação
\textbf{STOCHASTIC} não conseguiu encontrar uma política "perfeita" que acha o melhor caminho em direção ao objetivo. Em muitos estados, a melhor ação a se tomar calculada ao final do algoritmo não aparenta ser de fato a melhor. A variação
\textbf{POSITIVE} é ainda pior, as vezes achando caminhos opostos ao ótimo.

Vale a pena citar que, assim como no mapa teste, a variação \textbf{POSITIVE} demorou muito, e o número de episódios teve que ser diminuído
para obtermos uma resposta rápidamente.

\subsection{Mapa escolhas (\textit{Choices})}

O mapa \textit{choices} está exposto abaixo.

\begin{center}
	\[
		\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			@ & . & . & . & . & . & . & . & . & . & @ \\ \hline
			@ & + & @ & . & x & @ & x & . & @ & ; & @ \\ \hline
			@ & + & @ & . & @ & @ & x & . & @ & ; & @ \\ \hline
			@ & + & @ & . & x & @ & x & . & @ & ; & @ \\ \hline
			@ & + & . & . & @ & @ & x & . & . & ; & @ \\ \hline
			@ & + & @ & . & x & @ & x & . & @ & ; & @ \\ \hline
			@ & + & @ & . & @ & @ & x & . & @ & ; & @ \\ \hline
			@ & + & . & . & . & O & . & . & . & ; & @ \\ \hline
		\end{array}
	\]
\end{center}


\vspace{10pt}


\begin{center}
	\small \textbf{STANDARD}
	\[
		\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			@ & >      & > & >      & > & \cellcolor{green!20}> & > & v & < & <      & @ \\ \hline
			@ & \hat{} & @ & \hat{} & x & @                     & x & v & @ & \hat{} & @ \\ \hline
			@ & \hat{} & @ & v      & @ & @                     & x & v & @ & \hat{} & @ \\ \hline
			@ & \hat{} & @ & v      & x & @                     & x & v & @ & \hat{} & @ \\ \hline
			@ & >      & > & \hat{} & @ & @                     & x & v & < & <      & @ \\ \hline
			@ & <      & @ & v      & x & @                     & x & v & @ & \hat{} & @ \\ \hline
			@ & >      & @ & \hat{} & @ & @                     & x & v & @ & \hat{} & @ \\ \hline
			@ & v      & > & \hat{} & < & \cellcolor{red!20}O   & < & < & < & <      & @ \\ \hline
		\end{array}
	\]
\end{center}


\small \textbf{POSITIVE}
\[
	\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		@ & v      & > & v & < & \cellcolor{green!20}< & < & <      & < & <      & @ \\ \hline
		@ & v      & @ & v & x & @                     & x & v      & @ & >      & @ \\ \hline
		@ & v      & @ & v & @ & @                     & x & \hat{} & @ & v      & @ \\ \hline
		@ & v      & @ & v & x & @                     & x & <      & @ & >      & @ \\ \hline
		@ & >      & > & < & @ & @                     & x & <      & < & v      & @ \\ \hline
		@ & \hat{} & @ & v & x & @                     & x & <      & @ & \hat{} & @ \\ \hline
		@ & v      & @ & v & @ & @                     & x & v      & @ & >      & @ \\ \hline
		@ & >      & > & > & > & \cellcolor{red!20}O   & < & \hat{} & < & >      & @ \\ \hline
	\end{array}
\]

\small \textbf{STOCHASTIC}
\[
	\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		@ & >      & > & v      & v & \cellcolor{green!20}> & v & v      & < & < & @ \\ \hline
		@ & \hat{} & @ & >      & x & @                     & x & <      & @ & v & @ \\ \hline
		@ & v      & @ & v      & @ & @                     & x & <      & @ & v & @ \\ \hline
		@ & <      & @ & >      & x & @                     & x & <      & @ & v & @ \\ \hline
		@ & >      & > & \hat{} & @ & @                     & x & \hat{} & > & < & @ \\ \hline
		@ & \hat{} & @ & >      & x & @                     & x & \hat{} & @ & v & @ \\ \hline
		@ & \hat{} & @ & \hat{} & @ & @                     & x & <      & @ & v & @ \\ \hline
		@ & >      & > & \hat{} & < & \cellcolor{red!20}O   & > & \hat{} & > & < & @ \\ \hline
	\end{array}
\]

Algumas análises interessantes podem ser feitas nesse mapa. Mais uma vez, a variação \textbf{STANDARD} foi superior às outras e obteve uma política satisfatória. A variação \textbf{POSITIVE}, apesar de mais uma vez muito lenta,
também achou uma política satisfatória. O curioso nesse caso é que o caminho mais "indicado" pela política é o que desce para o objetivo pela esquerda, ao contrátio da variação anterior, que indicava que o melhor caminho era descer pela direita.
A variação \textbf{STOCHASTIC}, no entanto, teve um desempenho extremamente ruim nesse mapa. A política encontrada não apresenta qualquer tipo de sentido e não aponta boas escolhas para se chegar ao objetivo. Meu palpite inicial era de que talvez o número de iterações fosse muito baixo para que
o algoritmo desempenha-se bem, mas mesmo aumentando o número de episódio a performance não melhorou.


\section{Conclusão}

O trabalho foi uma excelente maneira de aprender mais e colocar em prática os conhecimentos adquiridos sobre aprendizado por reforço, em particular em relação ao algoritmo \textit{Q-Learning}. O algoritmo possui uma implementação simples e a linguagem \textit{Python} ajudou muito também,
por ser de fácil uso. Em relação às variações propostas, a \textbf{STANDARD} aparentou ser a melhor de todas, como esperado. A variação \textbf{STOCHASTIC} teve um desempenho bom nos primeiros testes, mas no último mapa os resultados não foram satisfatórios. Um estudo mais profundo sobre o caso desse mapa em específico
e como ocorreu o aprendizado do agente é necessário para compreender o que está acontecendo. A variação \textbf{POSITIVE}, por sua vez, até conseguia encontrar políticas interessantes para o agente, mas seu tempo de execução era extremamente lento, o que classificou essa variação negativamente.

\end{document}
