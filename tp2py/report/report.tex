\documentclass{article}
\usepackage{multicol}
\usepackage{subfig}

\usepackage{amsmath}
\usepackage{float}
\usepackage{algorithm}
\usepackage{multicol}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage[table]{xcolor}

\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Trabalho Prático II - Introdução à Inteligência Artificial}
\author{Luís Felipe Ramos Ferreira}
\date{\href{mailto:lframos.lf@gmail.com}{\texttt{lframos.lf@gmail.com}}
}

\begin{document}

\maketitle

\section{Introdução}

O Trabalho Prático II da disciplina de Introdução a Inteligência Artificial teve como objetivo a implementação do algoritmo de \texttt{Q-Learning}
para encontrar o melhor caminho entro um ponto inicial e um objetiov em uma mapa bidimensional.

\section{Implementação}

O projeto foi implementado na linguagem Python, versão 3.12.3. Um arquivo \texttt{requirements.txt} com os pacotes utilizados no ambiente virtual criado para desenvolvimento
está disponibilizado. O único pacote fora dos já disponibilizados por padrão na Linguagem foram Numpy, Pandas e Matplotlib. Instruções para rodar o programa estão disponibilizadas no
arquivo \texttt{README.md} disponibilizado.

\section{Q-learning}

O algoritmo de \texttt{Q-Learning} é um algoritmo de aprendizado por reforço, em que o agente, ao explorar o ambiente e interagir com o que ele possui,
passa a aprender mais sobre como alcançar seus objetivos. Para cada possível estado \(s\), ele seleciona a melhor ação \(a\) para se tomar, obtendo assim
o estado \(s'\) que será alcançado ao aplicar \(a\) em \(s\), assim como uma recompensa \(r\) por aplicar essa ação. Por fim, ele atualiza o valor de \(Q(s, a)\),
que é mantido para todo par de estado e ação e indica quão bom é executar aquela ação partindo daquele estado.

O pseudocódigo abaixo, inspirado nos slides da disciplina, indica, em alto nível, como funciona o algoritmo.

\begin{itemize}
	\item Inicializa \(Q(s, a)\) para todos os estados e ações
	\item \(s \gets\) estado inicial
	\item \(n \gets\) número de episódios
	\item \(\epsilon \gets\) limiar \(\epsilon-greedy\)
	\item \(\gamma \gets\) taxa de desconte
	\item \(\alpha \gets\) taxa de aprendizado
	\item Para todo \(i = 0\) até \(n\)
	      \begin{itemize}
		      \item se \(random() < \epsilon: a \gets random\_action()\)
		      \item senão \(a \gets argmax_a(Q(s, a))\)
		      \item Executa ação \(a\), observa recompensa \(r\) e próximo estado \(s'\)
		      \item \(Q(s, a) \gets Q(s, a) + \alpha * [r + \gamma* max_{a' \in A}Q(s', a') - Q(s, a)]\)
		      \item \(s \gets s'\)
	      \end{itemize}
\end{itemize}

Uma característica do \texttt{Q-Learning} é que ele é um algoritmo chamado de \textit{model free}, que em alto nível significa dizer que em nenhum momento o agente
precisa aprender ou estimar a função de transição que ele utiliza. Ele também é um algoritmo \textit{off-policy}, ou seja, a política utilizada pelo agente
na exploração pode ser diferente da política que ele está aprendendo (\textit{target policy}).

O algoritmo possui algumas pequenas variações, as quais exploramos no trabalho, e estão descritas a seguir.

\begin{itemize}
	\item \textbf{STANDARD}: nesta versão, utilizamos o algortimo normalmente, com a tabela de recompensas descrita na especificação.
	\item \textbf{POSITIVE}: também utilizamos a tabela descrita na especificação do trabalho. Nesa versão, toda recompensa é positiva.
	\item \textbf{STOCHASTIC}: nesta versão, após escolher uma ação, o algortimo tem 20\% de chance de na verdade executar outra ação perpendicular à ação escolhida,
	      conforme descrito na especificação.
\end{itemize}

\section{Estruturas e modelagem}

A linguagem \textit{Python} facilitou muito o trabalho. Para modelar as 4 ações possíveis, utilizamos números inteiros. EM particular, para ficar mais organizado,
utilizamos a estrutura \textit{IntEnum}. O mapa é um \textit{array} da biblioteca \textit{numpy}, que armazena caracteres. A matriz de pesos é uma matriz tridimensional,
também \textit{numpy}, que armazena floats. Ela possui as dimensões do mapa de entrada, mas cada posição é composta por um vetor de 4 posições, uma para cada possível ação.

As variações \textbf{STANDARD} e \textbf{POSITIVE} foram implementadas na mesma função, chamada de \textit{qlearning}, pois elas são idênticas a menos do valor das recompensas, que é mudado dinamicamente conforme
os parâmetros de linha de comando. A variação \textbf{STOCHASTIC} é implementada numa função diferente, chamada \textit{stochastic}, para facilitar a organização.

As implementações seguem exatamente as especificações do algoritmo apresentadas na disciplina.

\section{Análise das diferentes políticas}

Para analisar as diferentes políticas, testamos elas com os diferentes mapas disponibilizados no \textit{moodle} da disciplina. Os resultados obtidos foram os seguintes.
Nas tabelas, a célula verde indica o ponto inicial e a célula vermelha indica o ponto onde se encontra o objetivo.

\subsection{Mapa teste}

Para o mapa teste inicial, que pode ser visto abaixo, as políticas encontradas para cada uma das variações foram as seguintes.

\begin{center}
	\[
		\begin{array}{|c|c|c|c|c|}
			\hline
			+ & + & + & . & O \\ \hline
			. & @ & @ & . & x \\ \hline
			. & @ & @ & . & . \\ \hline
			. & ; & ; & ; & . \\ \hline
		\end{array}
	\]
\end{center}

\vspace{10pt}

\begin{multicols}{3}

	\centering \small \textbf{STANDARD}
	\[
		\begin{array}{|c|c|c|c|c|}
			\hline
			>                     & > & > & >      & \cellcolor{red!20}O \\ \hline
			v                     & @ & @ & \hat{} & x                   \\ \hline
			v                     & @ & @ & \hat{} & <                   \\ \hline
			\cellcolor{green!20}> & > & > & \hat{} & \hat{}              \\ \hline
		\end{array}
	\]

	\columnbreak

	\centering \small \textbf{POSITIVE}
	\[
		\begin{array}{|c|c|c|c|c|}
			\hline
			>                     & > & > & <      & \cellcolor{red!20}O \\ \hline
			v                     & @ & @ & \hat{} & x                   \\ \hline
			v                     & @ & @ & >      & v                   \\ \hline
			\cellcolor{green!20}> & > & > & \hat{} & \hat{}              \\ \hline
		\end{array}
	\]

	\columnbreak

	\centering \small \textbf{STOCHASTIC}
	\[
		\begin{array}{|c|c|c|c|c|}
			\hline
			>                          & > & > & >      & \cellcolor{red!20}O \\ \hline
			\hat{}                     & @ & @ & >      & x                   \\ \hline
			\hat{}                     & @ & @ & >      & <                   \\ \hline
			\cellcolor{green!20}\hat{} & > & > & \hat{} & \hat{}              \\ \hline
		\end{array}
	\]

\end{multicols}

A política foi encontrada para o ponto inciial \((0, 3)\), com 10000 episódios. EM particular, para a variação \textbf{POSITIVE}, que demora bastante, utilizamos 500 iterações. Podems ver que todas encontram uma política satisfatória
para encontrar o melhor caminho para o objetivo. Em particular, a variação \textbf{STANDARD} foi a mais rápida e a que, a princípio, encontrou o melhor resultado.

\subsection{Mapa labirinto (\textit{Maze})}

\[
	\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		x & @ & x & @ & x & @ & x & @ & x & @ & . & @ \\ \hline
		. & . & . & . & . & . & . & . & . & . & . & @ \\ \hline
		. & x & @ & @ & @ & @ & . & @ & . & @ & @ & @ \\ \hline
		. & x & @ & . & . & . & . & . & . & @ & . & @ \\ \hline
		. & x & @ & . & @ & @ & @ & @ & @ & @ & @ & @ \\ \hline
		. & x & @ & . & . & . & . & . & . & @ & . & . \\ \hline
		. & x & @ & @ & @ & @ & @ & @ & @ & @ & @ & . \\ \hline
		O & . & . & . & . & . & . & . & . & . & . & . \\ \hline
	\end{array}
\]

\vspace{10pt}


% Tabela 1
\centering \small \textbf{STANDARD}
\[
	\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		x                   & @ & x      & @      & x      & @        & x      & @ & x & @ & \cellcolor{green!20}v & @ \\ \hline
		v                   & < & <      & <      & <      & <        & <      & < & < & < & <                     & @ \\ \hline
		v                   & x & @      & @      & @      & @        & @      & @ & @ & @ & \hat{}                & @ \\ \hline
		v                   & x & @      & <      & <      & >        & <      & > & < & > & \hat{}                & @ \\ \hline
		v                   & x & @      & >      & @      & @        & @      & @ & @ & @ & @                     & @ \\ \hline
		v                   & x & @      & \hat{} & <      & <        & \hat{} & v & < & > & >                     & x \\ \hline
		v                   & x & @      & @      & @      & @        & @      & @ & @ & @ & >                     & @ \\ \hline
		\cellcolor{red!20}O & v & \hat{} & >      & \hat{} & \hat{} < & v      & > & > & @                             \\ \hline
	\end{array}
\]


% Tabela 2
\centering \small \textbf{POSITIVE}
\[
	\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		x                   & @      & x & @      & x      & @ & x      & @      & x      & @ & \cellcolor{green!20} v & @ \\ \hline
		v                   & <      & < & <      & <      & < & <      & <      & >      & < & <                      & @ \\ \hline
		\hat{}              & x      & @ & @      & @      & @ & @      & @      & @      & @ & \hat{}                 & @ \\ \hline
		\hat{}              & x      & @ & v      & \hat{} & v & v      & \hat{} & \hat{} & < & \hat{}                 & @ \\ \hline
		\hat{}              & x      & @ & \hat{} & @      & @ & @      & @      & @      & @ & @                      & @ \\ \hline
		\hat{}              & x      & @ & v      & >      & < & \hat{} & >      & \hat{} & v & \hat{}                 & x \\ \hline
		\hat{}              & x      & @ & @      & @      & @ & @      & @      & @      & @ & v                      & @ \\ \hline
		\cellcolor{red!20}O & \hat{} & < & v      & v      & > & v      & \hat{} & v      & < & v                      & @ \\ \hline
	\end{array}
\]


\centering \small \textbf{STOCHASTIC}
\[
	\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		x & @ & x & @ & x & @ & x & @ & x & @ & v & @
		> & < & < & > & ^ & < & ^ & > & ^ & < & < & @
		^ & x & @ & @ & @ & @ & @ & @ & @ & @ & ^ & @
		> & x & @ & v & < & < & < & v & > & > & ^ & @
		^ & x & @ & v & @ & @ & @ & @ & @ & @ & @ & @
		> & x & @ & > & > & > & > & > & > & > & > & x
		^ & x & @ & @ & @ & @ & @ & @ & @ & @ & ^ & @
		O & < & < & < & < & < & < & < & < & > & ^ & @
	\end{array}
\]

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		x & @ & x & @      & x      & @ & x      & @ & x      & @      & v \\ \hline
		v & < & < & <      & <      & < & <      & @ & <      & @      &   \\ \hline
		v & x & @ & @      & @      & @ & \hat{} & @ & ^      & @      &   \\ \hline
		v & x & @ & <      & <      & > & <      & > & \hat{} & v      & ^ \\ \hline
		v & x & @ & >      & @      & @ & @      & @ & @      & \hat{} &   \\ \hline
		v & x & @ & \hat{} & <      & < & ^      & v & <      & >      & x \\ \hline
		v & x & @ & @      & @      & @ & >      & @ & ^      & \hat{} &   \\ \hline
		O & v & ^ & >      & \hat{} & ^ & v      & < & v      & >      & @ \\ \hline
	\end{tabular}
	\caption{Primeira tabela}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		x      & @ & x & @      & x      & @ & x      & @      & x      & @      & v     \\ \hline
		v      & < & < & <      & <      & < & \hat{} & <      & <      & @      &       \\ \hline
		\hat{} & x & @ & @      & @      & v & \hat{} & v      & v      & \hat{} & < & @ \\ \hline
		\hat{} & x & @ & \hat{} & @      & @ & @      & @      & @      & \hat{} &       \\ \hline
		\hat{} & x & @ & v      & <      & ^ & >      & \hat{} & ^      & x      &       \\ \hline
		\hat{} & x & @ & >      & \hat{} & > & v      & \hat{} & ^      & v      & v     \\ \hline
		\hat{} & x & @ & <      & <      & @ & x      & \hat{} & v      & ^      & v     \\ \hline
		O      & ^ & < & v      & >      & v & v      & v      & \hat{} & \hat{} & ^     \\ \hline
	\end{tabular}
	\caption{Segunda tabela}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		x      & @ & x      & @      & x      & @ & x      & @      & x      & @      & v     \\ \hline
		\hat{} & < & >      & <      & ^      & < & >      & ^      & \hat{} & ^      & <     \\ \hline
		\hat{} & x & @      & @      & @      & @ & v      & \hat{} & v      & <      & < & ^ \\ \hline
		x      & @ & v      & \hat{} & v      & < & <      & ^      & v      & \hat{} & ^ & @ \\ \hline
		x      & @ & \hat{} & v      & \hat{} & v & \hat{} & v      & \hat{} & v      & ^     \\ \hline
		x      & @ & @      & @      & @      & v & ^      & v      & v      & <      & > & v \\ \hline
		\hat{} & x & @      & >      & @      & > & >      & v      & ^      & @      & @     \\ \hline
		O      & x & @      & >      & @      & @ & >      & v      & \hat{} & ^      & ^     \\ \hline
	\end{tabular}
	\caption{Terceira tabela}
\end{table}

\section{Conclusão}

the last dance

\end{document}
